# Manual Testing Checklist: Message Streaming (T024)

**Feature**: 009-message-streaming
**Date**: 2026-01-13
**User Story**: US1 - Real-Time Response Streaming

## Prerequisites

- Backend server running: `cd backend && python -m uvicorn main:app --reload`
- Frontend running: `cd frontend && npm run dev`
- Browser DevTools open (Network tab, Console)

---

## Test Cases

### 1. Basic Streaming Functionality

- [ ] **TC001**: Send a simple message ("Hello")
  - âœ… First token appears within 1 second
  - âœ… Tokens appear progressively (word-by-word or token-by-token)
  - âœ… Streaming indicator (blinking cursor) visible during streaming
  - âœ… Message completes and indicator disappears
  - âœ… Completed message saved in conversation history

- [ ] **TC002**: Send a longer message requesting detailed response
  - âœ… Streaming continues smoothly for longer responses
  - âœ… No lag or stuttering between tokens
  - âœ… Auto-scroll keeps latest tokens visible

### 2. Network & Protocol

- [ ] **TC003**: Verify SSE in DevTools Network tab
  - âœ… Request shows "EventStream" type
  - âœ… Accept header: `text/event-stream`
  - âœ… Response Content-Type: `text/event-stream`
  - âœ… Events visible in Network panel

- [ ] **TC004**: Check console logs for streaming events
  - âœ… "Stream started" log appears
  - âœ… Token received logs (every 10 tokens if debug enabled)
  - âœ… "Stream completed" log with duration and token count

### 3. Conversation History

- [ ] **TC005**: Send follow-up message with context
  - âœ… Second message includes history from first message
  - âœ… LLM responds with contextual awareness
  - âœ… Both messages saved in localStorage

- [ ] **TC006**: Reload page and verify history
  - âœ… Previous messages load from localStorage
  - âœ… Can send new streaming message with history

### 4. Model Selection

- [ ] **TC007**: Select different model (e.g., gpt-4)
  - âœ… Streaming works with selected model
  - âœ… Model indicator displays correct model name
  - âœ… Response quality matches selected model

- [ ] **TC008**: Switch models between messages
  - âœ… Each message uses its selected model
  - âœ… Model indicators show correct model per message

### 5. Error Handling

- [ ] **TC009**: Stop backend during streaming
  - âœ… Error message displays after connection loss
  - âœ… Partial response preserved if any tokens received
  - âœ… Error indicator visible

- [ ] **TC010**: Send empty message
  - âœ… Validation error prevents streaming start
  - âœ… Error message shown to user

- [ ] **TC011**: Test with rate limit (if applicable)
  - âœ… ErrorEvent received and displayed
  - âœ… Partial tokens preserved
  - âœ… User can retry after error

### 6. Special Characters & Unicode

- [ ] **TC012**: Request response with emojis
  - âœ… Emojis stream correctly (ðŸš€ ðŸŽ‰ âœ¨)
  - âœ… No encoding issues or broken characters

- [ ] **TC013**: Request response with non-Latin scripts
  - âœ… Unicode characters stream correctly (ä¸­æ–‡, Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©, ×¢×‘×¨×™×ª)
  - âœ… Characters display properly in UI

### 7. UI/UX

- [ ] **TC014**: Visual streaming indicators
  - âœ… Blinking cursor appears during streaming
  - âœ… Subtle pulse animation on streaming message
  - âœ… Indicators disappear when streaming completes

- [ ] **TC015**: Message bubble styling
  - âœ… Streaming messages have distinct styling
  - âœ… Completed messages return to normal styling
  - âœ… Timestamp displays correctly

- [ ] **TC016**: Auto-scroll behavior
  - âœ… Chat scrolls automatically as tokens arrive
  - âœ… User can scroll up during streaming without interference
  - âœ… Scroll position maintained for older messages

### 8. Concurrent & Edge Cases

- [ ] **TC017**: Send message while streaming
  - âœ… Cannot send new message while streaming (button disabled or warning shown)
  - âœ… OR: New message queued and sent after current stream completes

- [ ] **TC018**: Multiple rapid messages
  - âœ… Messages stream in sequence
  - âœ… No race conditions or overlapping streams

- [ ] **TC019**: Browser tab switch during streaming
  - âœ… Streaming continues in background
  - âœ… Tokens visible when returning to tab

### 9. Performance

- [ ] **TC020**: First token latency
  - âœ… Measure time from send to first token
  - âœ… Target: < 1 second (check console logs for timing)

- [ ] **TC021**: Stream throughput
  - âœ… Tokens arrive smoothly without noticeable delays
  - âœ… Check console: tokens per second metric in "Stream completed" log

### 10. Backward Compatibility

- [ ] **TC022**: Verify non-streaming still works
  - âœ… Old clients (if any) receive complete JSON response
  - âœ… API handles both `Accept: application/json` and `Accept: text/event-stream`

---

## Acceptance Criteria Validation

From spec.md User Story 1:

1. âœ… **First token within 1-2 seconds**: Verify with TC001, TC020
2. âœ… **Each token appears immediately**: Verify with TC001, TC002
3. âœ… **Completed messages saved**: Verify with TC001, TC005, TC006
4. âœ… **Message ordering maintained**: Verify with TC018

---

## Test Results

**Date Tested**: _____________
**Tester**: _____________
**Environment**: _____________

**Pass Rate**: _____ / 22 test cases

**Issues Found**:
1.
2.
3.

**Notes**:


---

## Curl Commands for Direct API Testing

```bash
# Test SSE endpoint directly
curl -N -H "Accept: text/event-stream" \
  -H "Content-Type: application/json" \
  -d '{"message":"Tell me a short story","timestamp":"2026-01-13T22:00:00Z"}' \
  http://localhost:8000/api/v1/messages

# Test JSON endpoint (backward compatibility)
curl -H "Accept: application/json" \
  -H "Content-Type: application/json" \
  -d '{"message":"Hello","timestamp":"2026-01-13T22:00:00Z"}' \
  http://localhost:8000/api/v1/messages
```

---

## Known Limitations

- T021-T023 (ChatArea update, integration tests, E2E tests) are not yet implemented
- Full integration of streaming into ChatArea component pending
- E2E tests with Playwright not yet created

**Status**: Backend streaming MVP complete (T001-T020), frontend UI components partially complete
